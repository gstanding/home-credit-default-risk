{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train shape:',train.shape)\n",
    "print('test shape:',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_, test_, y_, folds_):\n",
    "    train_ = pd.DataFrame()\n",
    "    test_ = pd.DataFrame()\n",
    "    oof_preds = np.zeros(train_.shape[0])\n",
    "    sub_preds = np.zeros(test_.shape[0])\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f_ for f_ in train.columns if f_ not in ['SK_ID_CURR']]\n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(train_)):\n",
    "        trn_x, trn_y = train_[[feats]].iloc[trn_idx], y_.iloc[trn_idx]\n",
    "        val_x, val_y = train_[[feats]].iloc[val_idx], y_.iloc[val_idx]\n",
    "        \n",
    "        clf = LGBMClassifier(\n",
    "            n_estimators = 4000,\n",
    "            learning_rate = 0.03,\n",
    "            num_leaves = 30,\n",
    "            colsample_bytree = .8,\n",
    "            subsample = .9,\n",
    "            max_depth = 7,\n",
    "            reg_alpha = .1,\n",
    "            min_split_gain = .01,\n",
    "            min_child_weight = 2,\n",
    "            silent = -1,\n",
    "            verbose = -1\n",
    "            )\n",
    "        clf.fit(trn_x, trn_y, \n",
    "            eval_set = [(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric = 'auc', verbose = 100, early_stopping_rounds = 100)\n",
    "        \n",
    "        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test[feats], num_iteration=clf.best_iteration_)[:, 1] / folds_.n_splits\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = feats\n",
    "        fold_importance_df['importance'] = clf.feature_importances_\n",
    "        fold_importance_df['fold'] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('fold %2d AUC %.6f'%(n_fold+1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "        del clf, trn_x, trn_y, val_x, val_y\n",
    "        gc.collect()\n",
    "    print('full AUC score %.6f'%roc_auc_score(y, oof_preds))\n",
    "    test_['TARGET'] = sub_preds\n",
    "    \n",
    "    return oof_preds, test_[['SK_ID_CURR', 'TARGET']], feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "#print(train.isnull().sum())\n",
    "test = test.fillna(-1)\n",
    "sm = SMOTE(random_state=42, kind='borderline2')\n",
    "train, y = sm.fit_sample(train, y)\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "oof_preds, test_preds, importances = train_model(train, test, y, folds)\n",
    "test_preds.to_csv('../data/submission2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'application_test.csv', 'application_train.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'readme.md', 'sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# data data files are available in the \"../data/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the data directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../data/\"))\n",
    "import gc\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def merge_data():\n",
    "    bur_bal = pd.read_csv('../data/bureau_balance.csv')\n",
    "    print('bureau_balance shape:', bur_bal.shape)\n",
    "    #bur_bal.head()\n",
    "    bur_bal = pd.concat([bur_bal, pd.get_dummies(bur_bal.STATUS, prefix='bur_bal_status')],\n",
    "                       axis=1).drop('STATUS', axis=1)\n",
    "    bur_cnts = bur_bal[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count()\n",
    "    bur_bal['bur_cnt'] = bur_bal['SK_ID_BUREAU'].map(bur_cnts['MONTHS_BALANCE'])\n",
    "    avg_bur_bal = bur_bal.groupby('SK_ID_BUREAU').mean()\n",
    "    avg_bur_bal.columns = ['bur_bal_' + f_ for f_ in avg_bur_bal.columns]\n",
    "    del bur_bal\n",
    "    gc.collect()\n",
    "\n",
    "    bur = pd.read_csv('../data/bureau.csv')\n",
    "    print('bureau shape:', bur.shape)\n",
    "    #bur.head()\n",
    "    bur_credit_active_dum = pd.get_dummies(bur.CREDIT_ACTIVE, prefix='ca')\n",
    "    bur_credit_currency_dum = pd.get_dummies(bur.CREDIT_CURRENCY, prefix='cc')\n",
    "    bur_credit_type_dum = pd.get_dummies(bur.CREDIT_TYPE, prefix='ct')\n",
    "\n",
    "    bur_full = pd.concat([bur, bur_credit_active_dum, bur_credit_currency_dum, bur_credit_type_dum], axis=1).drop(['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE'], axis=1)\n",
    "    del bur_credit_active_dum, bur_credit_currency_dum, bur_credit_type_dum\n",
    "    gc.collect()\n",
    "    bur_full = bur_full.merge(right=avg_bur_bal.reset_index(), how='left', on='SK_ID_BUREAU',suffixes=('', '_bur_bal'))\n",
    "    nb_bureau_per_curr = bur_full[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\n",
    "    bur_full['SK_ID_BUREAU'] = bur_full['SK_ID_CURR'].map(nb_bureau_per_curr['SK_ID_BUREAU'])\n",
    "    avg_bur = bur_full.groupby('SK_ID_CURR').mean()\n",
    "    avg_bur.columns = ['bur_' + f_ for f_ in avg_bur.columns]\n",
    "    del bur, bur_full, avg_bur_bal\n",
    "    gc.collect()\n",
    "\n",
    "    prev = pd.read_csv('../data/previous_application.csv')\n",
    "    print('previous_application shape:', prev.shape)\n",
    "    #prev.head()\n",
    "    prev_cat_features = [f_ for f_ in prev.columns if prev[f_].dtype == 'object']\n",
    "    prev_dum = pd.DataFrame()\n",
    "    for f_ in prev_cat_features:\n",
    "        prev_dum = pd.concat([prev_dum, pd.get_dummies(prev[f_], prefix=f_)], axis=1)\n",
    "    prev = pd.concat([prev, prev_dum],axis=1)\n",
    "    del prev_dum\n",
    "    gc.collect()\n",
    "    nb_prev_per_curr = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    prev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(nb_prev_per_curr['SK_ID_PREV'])\n",
    "    avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "    avg_prev.columns = ['prev_' + f_ for f_ in avg_prev.columns]\n",
    "    del prev\n",
    "    gc.collect()\n",
    "\n",
    "    pos = pd.read_csv('../data/POS_CASH_balance.csv')\n",
    "    print('pos_cash_balance shape:', pos.shape)\n",
    "    #pos.head()\n",
    "    pos = pd.concat([pos, pd.get_dummies(pos['NAME_CONTRACT_STATUS'], prefix='ncs')], axis=1)\n",
    "    nb_prevs = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    pos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    avg_pos = pos.groupby('SK_ID_CURR').mean()\n",
    "    avg_pos.columns = ['pos_' + f_ for f_ in avg_pos.columns]\n",
    "    del pos, nb_prevs\n",
    "    gc.collect()\n",
    "\n",
    "    cc_bal = pd.read_csv('../data/credit_card_balance.csv')\n",
    "    print('credit_card_balance shape:', cc_bal.shape)\n",
    "    cc_bal = pd.concat([cc_bal, pd.get_dummies(cc_bal['NAME_CONTRACT_STATUS'], prefix='ncs')], axis=1)\n",
    "    nb_prevs = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    cc_bal['SK_ID_PREV'] = cc_bal['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    avg_cc_bal = cc_bal.groupby('SK_ID_CURR').mean()\n",
    "    avg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n",
    "    del cc_bal, nb_prevs\n",
    "    gc.collect()\n",
    "\n",
    "    inst = pd.read_csv('../data/installments_payments.csv')\n",
    "    print('installment_payment shape:', inst.shape)\n",
    "    nb_prevs = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    inst['SK_ID_PREV'] = inst['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    avg_inst = inst.groupby('SK_ID_CURR').mean()\n",
    "    avg_inst.columns = ['inst_' + f_ for f_ in avg_inst.columns]\n",
    "    del inst, nb_prevs\n",
    "    gc.collect()\n",
    "\n",
    "    train = pd.read_csv('../data/application_train.csv')\n",
    "    test = pd.read_csv('../data/application_test.csv')\n",
    "    print('train shape:', train.shape)\n",
    "    print('test shape:', test.shape)\n",
    "    y = train['TARGET']\n",
    "    del train['TARGET']\n",
    "    cat_feats = [f_ for f_ in train.columns if train[f_].dtype == 'object']\n",
    "    for f_ in cat_feats:\n",
    "        train[f_], indexer = pd.factorize(train[f_])#类似于类似于类似于label encoder\n",
    "        test[f_] = indexer.get_indexer(test[f_])\n",
    "    train = train.merge(right = avg_bur.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right = avg_bur.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    train = train.merge(right = avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right = avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    train = train.merge(right = avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right = avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    train = train.merge(right = avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right = avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    train = train.merge(right = avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right = avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    del avg_bur, avg_prev, avg_pos, avg_cc_bal, avg_inst\n",
    "    gc.collect()\n",
    "    return train, test, y\n",
    "    \n",
    "def train_model(train_, test_, y_, folds_,feats_,features_):\n",
    "    train_ = pd.DataFrame(train_)\n",
    "    test_ = pd.DataFrame(test_)\n",
    "    print(train_.shape)\n",
    "    print(test_.shape)\n",
    "    train_.columns = feats_\n",
    "    test_.columns = features_\n",
    "    oof_preds = np.zeros(train_.shape[0])\n",
    "    sub_preds = np.zeros(test_.shape[0])\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    #feats = [f_ for f_ in train_.columns if f_ not in ['SK_ID_CURR']]\n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(train_)):\n",
    "        #print(train_.type)\n",
    "        trn_x, trn_y = pd.DataFrame(train_).iloc[trn_idx], pd.DataFrame(y_).iloc[trn_idx]\n",
    "        val_x, val_y = pd.DataFrame(train_).iloc[val_idx], pd.DataFrame(y_).iloc[val_idx]\n",
    "        \n",
    "        clf = LGBMClassifier(\n",
    "            n_estimators = 4000,\n",
    "            learning_rate = 0.03,\n",
    "            num_leaves = 30,\n",
    "            colsample_bytree = .8,\n",
    "            subsample = .9,\n",
    "            max_depth = 7,\n",
    "            reg_alpha = .1,\n",
    "            min_split_gain = .01,\n",
    "            min_child_weight = 2,\n",
    "            silent = -1,\n",
    "            verbose = -1\n",
    "            )\n",
    "        clf.fit(trn_x, trn_y, \n",
    "            eval_set = [(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric = 'auc', verbose = 100, early_stopping_rounds = 100)\n",
    "        \n",
    "        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(pd.DataFrame(test_[feats_]), num_iteration=clf.best_iteration_)[:, 1] / folds_.n_splits\n",
    "        feature_importance_df = 0\n",
    "        #fold_importance_df = pd.DataFrame()\n",
    "        #fold_importance_df['feature'] = feats\n",
    "        #fold_importance_df['importance'] = clf.feature_importances_\n",
    "        #fold_importance_df['fold'] = n_fold + 1\n",
    "        #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('fold %2d AUC %.6f'%(n_fold+1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "        del clf, trn_x, trn_y, val_x, val_y\n",
    "        gc.collect()\n",
    "    print('full AUC score %.6f'%roc_auc_score(y, oof_preds))\n",
    "    test_['TARGET'] = sub_preds\n",
    "    \n",
    "    return oof_preds, test_[['SK_ID_CURR', 'TARGET']], feature_importance_df\n",
    "    \n",
    "def display_importance(feature_importance_df_, num):\n",
    "    cols = feature_importance_df_[['feature', 'importance']].groupby('feature').mean().sort_values(by='importance', ascending=False)[:num].index\n",
    "    best_features = feature_importance_df_[feature_importance_df_.feature.isin(cols)]\n",
    "    \n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x='importance', y='feature',\n",
    "        data = best_features.sort_values(by = 'importance', ascending=False))\n",
    "    plt.title('LightGBM Feature(average over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "    \n",
    "    return cols\n",
    "    \n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def feat_ext_source(df):\n",
    "    x1 = df['EXT_SOURCE_1'].fillna(-1) + 1e-1\n",
    "    x2 = df['EXT_SOURCE_2'].fillna(-1) + 1e-1\n",
    "    x3 = df['EXT_SOURCE_3'].fillna(-1) + 1e-1\n",
    "    \n",
    "    df['EXT_SOURCE_1over2_NAminus1_Add0.1'] = x1/x2\n",
    "    df['EXT_SOURCE_2over1_NAminus1_Add0.1'] = x2/x1\n",
    "    df['EXT_SOURCE_1over3_NAminus1_Add0.1'] = x1/x3\n",
    "    df['EXT_SOURCE_3over1_NAminus1_Add0.1'] = x3/x1\n",
    "    df['EXT_SOURCE_2over3_NAminus1_Add0.1'] = x2/x3\n",
    "    df['EXT_SOURCE_3over2_NAminus1_Add0.1'] = x3/x2\n",
    "    df['EXT_SOURCE_1_log'] = np.log(df['EXT_SOURCE_1'] + 1)\n",
    "    df['EXT_SOURCE_2_log'] = np.log(df['EXT_SOURCE_2'] + 1)\n",
    "    df['EXT_SOURCE_3_log'] = np.log(df['EXT_SOURCE_3'] + 1) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau_balance shape: (27299925, 3)\n",
      "bureau shape: (1716428, 17)\n",
      "previous_application shape: (1670214, 37)\n",
      "pos_cash_balance shape: (10001358, 8)\n",
      "credit_card_balance shape: (3840312, 23)\n",
      "installment_payment shape: (13605401, 8)\n",
      "train shape: (307511, 122)\n",
      "test shape: (48744, 121)\n",
      "Memory usage of dataframe is 893.87 MB\n",
      "Memory usage after optimization is: 231.09 MB\n",
      "Decreased by 74.1%\n",
      "Memory usage of dataframe is 141.69 MB\n",
      "Memory usage after optimization is: 36.63 MB\n",
      "Decreased by 74.1%\n"
     ]
    }
   ],
   "source": [
    "gc.enable()\n",
    "train, test, y = merge_data()\n",
    "train = feat_ext_source(train)\n",
    "test = feat_ext_source(test)\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "#train.to_csv('train.csv', index=False)\n",
    "#test.to_csv('test.csv', index=False)\n",
    "#train.isnull().sum()\n",
    "train = train.fillna(-1)\n",
    "#print(train.isnull().sum())\n",
    "test = test.fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 389) (48744, 389)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f_ for f_ in train.columns if f_ not in ['SK_ID_CURR']]\n",
    "features = train.columns\n",
    "train = train[feats]\n",
    "#test = test[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 388) (48744, 389)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42, kind='borderline2')\n",
    "train, y = sm.fit_sample(train, y)\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(565372, 388)\n",
      "(48744, 389)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.977122\tvalid_1's auc: 0.97665\n",
      "[200]\ttraining's auc: 0.979736\tvalid_1's auc: 0.978764\n",
      "[300]\ttraining's auc: 0.981421\tvalid_1's auc: 0.979774\n",
      "[400]\ttraining's auc: 0.982649\tvalid_1's auc: 0.980254\n",
      "[500]\ttraining's auc: 0.983691\tvalid_1's auc: 0.980523\n",
      "[600]\ttraining's auc: 0.984652\tvalid_1's auc: 0.98067\n",
      "[700]\ttraining's auc: 0.985553\tvalid_1's auc: 0.980715\n",
      "[800]\ttraining's auc: 0.9864\tvalid_1's auc: 0.980766\n",
      "[900]\ttraining's auc: 0.987195\tvalid_1's auc: 0.980778\n",
      "[1000]\ttraining's auc: 0.987952\tvalid_1's auc: 0.980784\n",
      "Early stopping, best iteration is:\n",
      "[963]\ttraining's auc: 0.987673\tvalid_1's auc: 0.980793\n",
      "fold  1 AUC 0.980793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.977124\tvalid_1's auc: 0.976613\n",
      "[200]\ttraining's auc: 0.979692\tvalid_1's auc: 0.978754\n",
      "[300]\ttraining's auc: 0.981358\tvalid_1's auc: 0.979812\n",
      "[400]\ttraining's auc: 0.982583\tvalid_1's auc: 0.980373\n",
      "[500]\ttraining's auc: 0.983642\tvalid_1's auc: 0.980675\n",
      "[600]\ttraining's auc: 0.984601\tvalid_1's auc: 0.980874\n",
      "[700]\ttraining's auc: 0.985482\tvalid_1's auc: 0.981016\n",
      "[800]\ttraining's auc: 0.986343\tvalid_1's auc: 0.981083\n",
      "[900]\ttraining's auc: 0.987143\tvalid_1's auc: 0.981125\n",
      "[1000]\ttraining's auc: 0.987865\tvalid_1's auc: 0.981159\n",
      "[1100]\ttraining's auc: 0.988568\tvalid_1's auc: 0.981177\n",
      "[1200]\ttraining's auc: 0.989211\tvalid_1's auc: 0.981209\n",
      "[1300]\ttraining's auc: 0.989831\tvalid_1's auc: 0.981231\n",
      "Early stopping, best iteration is:\n",
      "[1268]\ttraining's auc: 0.989639\tvalid_1's auc: 0.981234\n",
      "fold  2 AUC 0.981234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.977122\tvalid_1's auc: 0.976519\n",
      "[200]\ttraining's auc: 0.979703\tvalid_1's auc: 0.978621\n",
      "[300]\ttraining's auc: 0.981355\tvalid_1's auc: 0.979701\n",
      "[400]\ttraining's auc: 0.982612\tvalid_1's auc: 0.980252\n",
      "[500]\ttraining's auc: 0.98368\tvalid_1's auc: 0.98052\n",
      "[600]\ttraining's auc: 0.984647\tvalid_1's auc: 0.98067\n",
      "[700]\ttraining's auc: 0.985565\tvalid_1's auc: 0.980761\n",
      "[800]\ttraining's auc: 0.986399\tvalid_1's auc: 0.980824\n",
      "[900]\ttraining's auc: 0.987176\tvalid_1's auc: 0.980871\n",
      "[1000]\ttraining's auc: 0.987908\tvalid_1's auc: 0.980889\n",
      "[1100]\ttraining's auc: 0.988578\tvalid_1's auc: 0.980924\n",
      "[1200]\ttraining's auc: 0.989201\tvalid_1's auc: 0.980923\n",
      "Early stopping, best iteration is:\n",
      "[1145]\ttraining's auc: 0.98887\tvalid_1's auc: 0.980938\n",
      "fold  3 AUC 0.980938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.976968\tvalid_1's auc: 0.977021\n",
      "[200]\ttraining's auc: 0.979635\tvalid_1's auc: 0.979267\n",
      "[300]\ttraining's auc: 0.98133\tvalid_1's auc: 0.98026\n",
      "[400]\ttraining's auc: 0.982555\tvalid_1's auc: 0.980751\n",
      "[500]\ttraining's auc: 0.983619\tvalid_1's auc: 0.981021\n",
      "[600]\ttraining's auc: 0.984589\tvalid_1's auc: 0.981166\n",
      "[700]\ttraining's auc: 0.985486\tvalid_1's auc: 0.981237\n",
      "[800]\ttraining's auc: 0.986321\tvalid_1's auc: 0.981276\n",
      "[900]\ttraining's auc: 0.987132\tvalid_1's auc: 0.981295\n",
      "[1000]\ttraining's auc: 0.987896\tvalid_1's auc: 0.981315\n",
      "[1100]\ttraining's auc: 0.988608\tvalid_1's auc: 0.981324\n",
      "[1200]\ttraining's auc: 0.989298\tvalid_1's auc: 0.981322\n",
      "Early stopping, best iteration is:\n",
      "[1107]\ttraining's auc: 0.98866\tvalid_1's auc: 0.981327\n",
      "fold  4 AUC 0.981327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\KG\\software\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.977157\tvalid_1's auc: 0.976387\n",
      "[200]\ttraining's auc: 0.979773\tvalid_1's auc: 0.978476\n",
      "[300]\ttraining's auc: 0.981424\tvalid_1's auc: 0.97948\n",
      "[400]\ttraining's auc: 0.982672\tvalid_1's auc: 0.979985\n",
      "[500]\ttraining's auc: 0.983728\tvalid_1's auc: 0.980248\n",
      "[600]\ttraining's auc: 0.984682\tvalid_1's auc: 0.980429\n",
      "[700]\ttraining's auc: 0.985629\tvalid_1's auc: 0.980522\n",
      "[800]\ttraining's auc: 0.98647\tvalid_1's auc: 0.980606\n",
      "[900]\ttraining's auc: 0.987264\tvalid_1's auc: 0.980665\n",
      "[1000]\ttraining's auc: 0.98801\tvalid_1's auc: 0.980693\n",
      "[1100]\ttraining's auc: 0.988705\tvalid_1's auc: 0.980715\n",
      "[1200]\ttraining's auc: 0.989335\tvalid_1's auc: 0.980733\n",
      "[1300]\ttraining's auc: 0.989948\tvalid_1's auc: 0.98074\n",
      "[1400]\ttraining's auc: 0.990537\tvalid_1's auc: 0.980767\n",
      "[1500]\ttraining's auc: 0.991076\tvalid_1's auc: 0.980771\n",
      "[1600]\ttraining's auc: 0.991586\tvalid_1's auc: 0.980777\n",
      "Early stopping, best iteration is:\n",
      "[1544]\ttraining's auc: 0.991309\tvalid_1's auc: 0.980789\n",
      "fold  5 AUC 0.980789\n",
      "full AUC score 0.981012\n"
     ]
    }
   ],
   "source": [
    "oof_preds, test_preds, importances = train_model(train, test, y, folds,feats, features)\n",
    "test_preds.to_csv('../data/submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_org = pd.read_csv('../data/test.csv')\n",
    "test_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred['SK_ID_CURR'] = test_org['SK_ID_CURR']\n",
    "test_pred['TARGET'] = test_['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_count = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count['features'] = train.columns\n",
    "nan_count['nan_nums'] = list(train.isnull().sum())\n",
    "nan_count.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isnull().sum()\n",
    "print(nan_count[nan_count['nan_nums'] > 155000].shape)\n",
    "nan_count[nan_count['nan_nums'] > 155000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
    "                    [3, 4, np.nan, 1],\n",
    "                    [np.nan, np.nan, np.nan, 5],\n",
    "                    [np.nan, 3, np.nan, 4]],\n",
    "                    columns=list('ABCD'))\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
